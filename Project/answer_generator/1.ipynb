{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af91a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, AsyncGenerator\n",
    "from collections import Counter\n",
    "import re\n",
    "import asyncio \n",
    "\n",
    "with open('apikeys.txt', 'r', encoding='utf-8') as file:\n",
    "    key1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b19cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandType(Enum):\n",
    "    FILE_QUERY = auto()\n",
    "    QA = auto()\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Internal representation of a document in your KB.\"\"\"\n",
    "    file_id: str\n",
    "    file_name: str\n",
    "    title: str\n",
    "    summary: str = \"\"\n",
    "    keywords: List[str] = field(default_factory=list)\n",
    "    text_length: int = 0\n",
    "    file_type: str = \"pdf\"     # text / pdf / image / others\n",
    "    extra_fields: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    \"\"\"Search result after enrichment.\"\"\"\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    relevance: float  # in percent\n",
    "    summary: str\n",
    "    key_fields_summary: str\n",
    "    high_freq_terms: Dict[str, int]\n",
    "\n",
    "@dataclass\n",
    "class LLMConfig:\n",
    "    \"\"\"LLM-related runtime config.\"\"\"\n",
    "    model: str = \"deepseek-chat\"\n",
    "    max_tokens: int = 512\n",
    "    api_key: Optional[str] = None\n",
    "    temperature: float = 0.2\n",
    "    base_url: str = \"https://api.deepseek.com\"\n",
    "\n",
    "def mock_documents() -> List[Document]:\n",
    "    return [\n",
    "        Document(\n",
    "            file_id=\"75ac7d52\",\n",
    "            file_name=\"Attention Is All You Need.pdf\",\n",
    "            title=\"Attention Is All You Need: The Transformer Revolution in Sequence Modeling\",\n",
    "            summary=(\n",
    "                \"This paper introduces the Transformer, a neural network architecture based solely on attention, \"\n",
    "                \"removing recurrence and convolution, enabling high parallelization and SOTA results on WMT14.\"\n",
    "            ),\n",
    "            keywords=[\n",
    "                \"transformer\",\n",
    "                \"attention\",\n",
    "                \"sequence\",\n",
    "                \"machine translation\",\n",
    "                \"parallelization\",\n",
    "            ],\n",
    "            text_length=39448,\n",
    "            file_type=\"pdf\",\n",
    "            extra_fields={\"author\": \"Vaswani et al.\", \"year\": \"2017\"},\n",
    "        ),\n",
    "        Document(\n",
    "            file_id=\"9f3a21aa\",\n",
    "            file_name=\"Company Policy 2025.pdf\",\n",
    "            title=\"Company Policy and Approval Flow 2025\",\n",
    "            summary=(\n",
    "                \"This document describes internal approval flows, reimbursement rules, HR processes, and policy updates for 2025.\"\n",
    "            ),\n",
    "            keywords=[\"policy\", \"approval\", \"hr\", \"reimbursement\"],\n",
    "            text_length=6800,\n",
    "            file_type=\"pdf\",\n",
    "            extra_fields={\"department\": \"HR\", \"created_at\": \"2025-10-20\"},\n",
    "        ),\n",
    "        Document(\n",
    "            file_id=\"abcdef\",\n",
    "            file_name=\"Deep Reinforcement Curriculum\",\n",
    "            title=\"Deep Reinforcement Learning & Curriculum Learning\",\n",
    "            summary=(\n",
    "                \"This fake document introduces deep learning, reinforcement learning, curriculum learning.\"\n",
    "            ),\n",
    "            keywords=[\"deep learning\", \"reinforcement learning\", \"curriculum\"],\n",
    "            text_length=1000,\n",
    "            file_type=\"pdf\",\n",
    "            extra_fields={},\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f9bb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "User query: Find me documents about RAG-based LLM question answering system design.\n",
      "→ Detected intent: FILE_QUERY\n",
      "→ Top matched documents:\n",
      "  - Attention Is All You Need: The Transformer Revolution in Sequence Modeling (relevance=100.00%)\n",
      "\n",
      "==============================\n",
      "User query: What is the difference between RAG and standard LLM QA? Please explain based on documents.\n",
      "→ Detected intent: QA\n",
      "→ Answer:\n",
      "No valid reference.\n",
      "\n",
      "==============================\n",
      "User query: Show me the company policy for 2025.\n",
      "→ Detected intent: FILE_QUERY\n",
      "→ Top matched documents:\n",
      "  - Company Policy and Approval Flow 2025 (relevance=100.00%)\n",
      "  - Attention Is All You Need: The Transformer Revolution in Sequence Modeling (relevance=26.73%)\n",
      "\n",
      "==============================\n",
      "User query: Explain the Transformer architecture.\n",
      "→ Detected intent: QA\n",
      "→ Answer:\n",
      "The Transformer architecture uses stacked encoder and decoder layers with multi-head self-attention mechanisms and position-wise feed-forward networks, eliminating recurrence and convolutions [75ac7d52].\n"
     ]
    }
   ],
   "source": [
    "class SemanticService:\n",
    "    \"\"\"\n",
    "    A small semantic layer that can:\n",
    "    1. Decide intent (FILE_QUERY vs QA) automatically.\n",
    "    2. Always search documents first.\n",
    "    3. If it's a file query -> return ranked docs.\n",
    "    4. If it's QA -> call LLM with retrieved context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents: Optional[List[Document]] = None):\n",
    "        self._documents: List[Document] = documents or mock_documents()\n",
    "\n",
    "        self._current_demand_raw: str = \"\"\n",
    "        self._current_demand_type: Optional[DemandType] = None\n",
    "        self._current_query_results: List[QueryResult] = []\n",
    "        self._stopped: bool = False\n",
    "\n",
    "        self._llm_config: LLMConfig = LLMConfig(api_key=key1)\n",
    "        self._client = self._build_client(self._llm_config)\n",
    "\n",
    "    # ======================\n",
    "    # Public API\n",
    "    # ======================\n",
    "\n",
    "    def set_demand(self, user_input: str) -> bool:\n",
    "        self._stopped = False\n",
    "        self._current_demand_raw = user_input.strip()\n",
    "        self._current_demand_type = self._classify_demand(user_input)\n",
    "        self._current_query_results = self._search_and_enrich(user_input)\n",
    "        return True\n",
    "\n",
    "    def stop_current_task(self) -> bool:\n",
    "        \"\"\"Frontend can call this to stop streaming.\"\"\"\n",
    "        self._stopped = True\n",
    "        return True\n",
    "\n",
    "    def redo_task(self, user_input: str) -> bool:\n",
    "        \"\"\"Re-run with new input.\"\"\"\n",
    "        return self.set_demand(user_input)\n",
    "\n",
    "    def get_query_file(self) -> List[str]:\n",
    "        \"\"\"Return only the titles for UI list.\"\"\"\n",
    "        return [r.title for r in self._current_query_results]\n",
    "\n",
    "    def get_qualified_files_info(self, top_n: int = 5) -> List[Dict[str, str]]:\n",
    "        \"\"\"Return structured info for top N results.\"\"\"\n",
    "        results: List[Dict[str, str]] = []\n",
    "        for r in self._current_query_results[:top_n]:\n",
    "            results.append({\n",
    "                \"doc_id\": r.doc_id,\n",
    "                \"title\": r.title,\n",
    "                \"relevance_percent\": f\"{r.relevance:.2f}%\",\n",
    "                \"summary\": r.summary,\n",
    "                \"key_fields_summary\": r.key_fields_summary,\n",
    "                \"high_freq_terms\": \", \".join([f\"{k}:{v}\" for k, v in r.high_freq_terms.items()]),\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def get_query_task_result(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Return all results (for debugging / inspection).\"\"\"\n",
    "        return self.get_qualified_files_info(top_n=len(self._current_query_results))\n",
    "\n",
    "    def get_LLM_reply(self) -> Any:\n",
    "        if not self._current_demand_raw:\n",
    "            return {\"error\": \"no demand set\"}\n",
    "\n",
    "        if self._current_demand_type == DemandType.FILE_QUERY:\n",
    "            return {\n",
    "                \"type\": \"file_query\",\n",
    "                \"query\": self._current_demand_raw,\n",
    "                \"results\": self.get_qualified_files_info(top_n=10),\n",
    "            }\n",
    "        \n",
    "        if not self._llm_config.api_key:\n",
    "            return {\"error\": \"QA without api key\"}\n",
    "    \n",
    "        context_text = self._build_context_from_results(self._current_query_results)\n",
    "        prompt = self._build_llm_prompt(\n",
    "            query=self._current_demand_raw,\n",
    "            context=context_text\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            resp = self._client.chat.completions.create(\n",
    "                model=self._llm_config.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                max_tokens=self._llm_config.max_tokens,\n",
    "                temperature=self._llm_config.temperature,\n",
    "            )\n",
    "            reply_text = resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            reply_text = f\"(LLM call failed) {e}\"\n",
    "\n",
    "        return {\n",
    "            \"type\": \"qa\",\n",
    "            \"prompt_sent\": prompt,\n",
    "            \"reply\": reply_text,\n",
    "        }\n",
    "\n",
    "    async def stream_LLM_reply(self) -> AsyncGenerator[str, None]:\n",
    "        \"\"\"\n",
    "        Async streaming version.\n",
    "        - respects stop flag\n",
    "        - works even with fake LLM reply\n",
    "        \"\"\"\n",
    "        reply_obj = self.get_LLM_reply()\n",
    "        text = reply_obj.get(\"reply\", \"\")\n",
    "        for ch in text:\n",
    "            if self._stopped:\n",
    "                break\n",
    "            yield ch\n",
    "            await asyncio.sleep(0.01)\n",
    "\n",
    "    def set_llm_config(\n",
    "        self,\n",
    "        *,\n",
    "        model: Optional[str] = None,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        base_url: Optional[str] = None,\n",
    "    ) -> bool:\n",
    "        \"\"\"Update LLM runtime config and rebuild client if needed.\"\"\"\n",
    "        if model is not None:\n",
    "            self._llm_config.model = model\n",
    "        if max_tokens is not None:\n",
    "            self._llm_config.max_tokens = max_tokens\n",
    "        if api_key is not None:\n",
    "            self._llm_config.api_key = api_key\n",
    "        if temperature is not None:\n",
    "            self._llm_config.temperature = temperature\n",
    "        if base_url is not None:\n",
    "            self._llm_config.base_url = base_url\n",
    "\n",
    "        # rebuild client every time api_key/base_url changes\n",
    "        self._client = self._build_client(self._llm_config)\n",
    "        return True\n",
    "\n",
    "    # ======================\n",
    "    # Internal: LLM & Intent\n",
    "    # ======================\n",
    "\n",
    "    def _build_client(self, cfg: LLMConfig) -> Optional[OpenAI]:\n",
    "        if not cfg.api_key:\n",
    "            return None\n",
    "        return OpenAI(api_key=cfg.api_key, base_url=cfg.base_url)\n",
    "\n",
    "    def _classify_demand(self, user_input: str) -> DemandType:\n",
    "        label = self._classify_with_llm(user_input)\n",
    "        if label is not None:\n",
    "            if label == \"FILE\":\n",
    "                return DemandType.FILE_QUERY\n",
    "            if label == \"QA\":\n",
    "                return DemandType.QA\n",
    "\n",
    "        # fallback: keyword-based\n",
    "        text = user_input.lower()\n",
    "        file_keywords = [\"file\", \"document\", \"doc\", \"list\", \"show\", \"open\", \"report\", \"pdf\", \"find\", \"search\"]\n",
    "        qa_keywords = [\"why\", \"how\", \"explain\", \"difference\", \"compare\", \"what is\", \"what's\"]\n",
    "\n",
    "        has_file = any(k in text for k in file_keywords)\n",
    "        has_qa = any(k in text for k in qa_keywords)\n",
    "\n",
    "        # prefer FILE\n",
    "        if has_file:\n",
    "            return DemandType.FILE_QUERY\n",
    "        if has_qa:\n",
    "            return DemandType.QA\n",
    "        return DemandType.FILE_QUERY\n",
    "\n",
    "    def _classify_with_llm(self, user_input: str) -> Optional[str]:\n",
    "        if not self._client or not self._llm_config.api_key:\n",
    "            return None\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an intent classifier. \"\n",
    "            \"You must answer with EXACTLY ONE WORD: 'FILE' or 'QA'. \"\n",
    "            \"Do NOT explain.\\n\"\n",
    "            \"- If the user wants to search/list/view/find/open documents/files/reports -> answer FILE.\\n\"\n",
    "            \"- If the user asks for explanation/analysis/how-to/reasoning -> answer QA.\\n\"\n",
    "            \"- If it is mixed, prefer FILE.\"\n",
    "        )\n",
    "        user_prompt = f\"User query:\\n{user_input}\\n\\nYour answer (FILE or QA):\"\n",
    "\n",
    "        try:\n",
    "            resp = self._client.chat.completions.create(\n",
    "                model=self._llm_config.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                max_tokens=4,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "            raw = raw.replace(\".\", \"\").strip().upper()\n",
    "            if raw in (\"FILE\", \"QA\"):\n",
    "                return raw\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # ======================\n",
    "    # Internal: Search Layer\n",
    "    # ======================\n",
    "\n",
    "    def _search_and_enrich(self, query: str) -> List[QueryResult]:\n",
    "        \"\"\"Very simple term-based search + enrichment.\"\"\"\n",
    "        query_tokens = self._tokenize(query)\n",
    "        results: List[QueryResult] = []\n",
    "\n",
    "        scored_docs = []\n",
    "        for doc in self._documents:\n",
    "            score = self._compute_relevance(query_tokens, doc)\n",
    "            if score > 0:\n",
    "                scored_docs.append((doc, score))\n",
    "\n",
    "        if not scored_docs:\n",
    "            return []\n",
    "\n",
    "        max_score = max(s for _, s in scored_docs) or 1.0\n",
    "\n",
    "        for doc, score in scored_docs:\n",
    "            relevance_percent = (score / max_score) * 100.0\n",
    "            summary = self._summarize_document(doc)\n",
    "            key_fields_summary = self._summarize_key_fields(doc)\n",
    "            high_freq_terms = self._extract_high_freq_terms(doc, query_tokens)\n",
    "\n",
    "            results.append(\n",
    "                QueryResult(\n",
    "                    doc_id=doc.file_id,\n",
    "                    title=doc.title,\n",
    "                    relevance=relevance_percent,\n",
    "                    summary=summary,\n",
    "                    key_fields_summary=key_fields_summary,\n",
    "                    high_freq_terms=high_freq_terms,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        results.sort(key=lambda x: x.relevance, reverse=True)\n",
    "        return results\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        # split on non-alphanum; include basic CJK if needed later\n",
    "        return [t for t in re.split(r\"[^0-9a-zA-Z]+\", text.lower()) if t]\n",
    "\n",
    "    def _compute_relevance(self, query_tokens: List[str], doc: Document) -> float:\n",
    "        if not query_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        doc_tokens = self._tokenize(\n",
    "            doc.summary + \" \" + doc.title + \" \" + \" \".join(doc.keywords)\n",
    "        )\n",
    "\n",
    "        q_set = set(query_tokens)\n",
    "        d_set = set(doc_tokens)\n",
    "\n",
    "        # 1) keyword overlap\n",
    "        keyword_matches = len(q_set & d_set)\n",
    "        keyword_score = keyword_matches / len(q_set)\n",
    "\n",
    "        # 2) tag bonus\n",
    "        tag_matches = 0\n",
    "        for t in doc.keywords:\n",
    "            if t.lower() in q_set:\n",
    "                tag_matches += 1\n",
    "        tag_score = min(tag_matches, 2) * 0.2  # cap\n",
    "\n",
    "        # 3) title bonus\n",
    "        title_tokens = self._tokenize(doc.title)\n",
    "        title_matches = len(q_set & set(title_tokens))\n",
    "        title_score = title_matches * 0.3\n",
    "\n",
    "        total_score = keyword_score * 0.6 + tag_score + title_score\n",
    "        return total_score\n",
    "\n",
    "    # ======================\n",
    "    # Internal: Summaries\n",
    "    # ======================\n",
    "\n",
    "    def _summarize_document(self, doc: Document) -> str:\n",
    "        \"\"\"Return a human-readable summary; now all-English.\"\"\"\n",
    "        if doc.file_type == \"text\":\n",
    "            short = doc.summary[:120]\n",
    "            return short + (\"...\" if len(doc.summary) > 120 else \"\")\n",
    "        else:\n",
    "            base = f\"This file is of type '{doc.file_type}', with title '{doc.title}'.\"\n",
    "            if doc.extra_fields:\n",
    "                base += \" Extra fields: \" + \", \".join(\n",
    "                    [f\"{k}: {v}\" for k, v in doc.extra_fields.items()]\n",
    "                )\n",
    "            return base\n",
    "\n",
    "    def _summarize_key_fields(self, doc: Document) -> str:\n",
    "        if not doc.extra_fields:\n",
    "            return \"No key fields.\"\n",
    "        return \"; \".join([f\"{k}={v}\" for k, v in doc.extra_fields.items()])\n",
    "\n",
    "    def _extract_high_freq_terms(\n",
    "        self,\n",
    "        doc: Document,\n",
    "        query_tokens: List[str],\n",
    "        top_k: int = 5\n",
    "    ) -> Dict[str, int]:\n",
    "        all_tokens = (\n",
    "            self._tokenize(doc.summary)\n",
    "            + query_tokens\n",
    "            + [t.lower() for t in doc.keywords]\n",
    "        )\n",
    "        counter = Counter(all_tokens)\n",
    "        return dict(counter.most_common(top_k))\n",
    "\n",
    "    # ======================\n",
    "    # Internal: Prompt\n",
    "    # ======================\n",
    "\n",
    "    def _build_context_from_results(self, results: List[QueryResult]) -> str:\n",
    "        blocks = []\n",
    "        for r in results:\n",
    "            blocks.append(f\"[{r.doc_id}] {r.title}\\n{r.summary}\\n\")\n",
    "        return \"\\n\".join(blocks)\n",
    "\n",
    "    def _build_llm_prompt(self, *, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        English version of your previous rule-based prompt to avoid hallucination.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are an enterprise internal knowledge-base assistant. You can ONLY use the information in the following DOCUMENTS to answer the user's question. If the documents do not contain enough information, you MUST answer: \"No valid reference.\"\n",
    "\n",
    "[Answering rules]\n",
    "1. Be concise and accurate.\n",
    "2. If you cite a document, add its id in square brackets at the end of the sentence, e.g. [75ac7d52].\n",
    "3. Do NOT invent information that is not in the documents.\n",
    "4. If multiple documents mention the same thing, you can merge them and cite multiple ids, e.g. [75ac7d52][9f3a21aa].\n",
    "\n",
    "[DOCUMENTS]\n",
    "{context}\n",
    "\n",
    "[USER QUESTION]\n",
    "{query}\n",
    "\n",
    "Start answering now:\n",
    "\"\"\".strip()\n",
    "        return prompt\n",
    "\n",
    "# ======================\n",
    "# Demo\n",
    "# ======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    service = SemanticService()\n",
    "\n",
    "    # just some example user queries\n",
    "    user_queries = [\n",
    "        \"Find me documents about RAG-based LLM question answering system design.\",\n",
    "        \"What is the difference between RAG and standard LLM QA? Please explain based on documents.\",\n",
    "        \"Show me the company policy for 2025.\",\n",
    "        \"Explain the Transformer architecture.\",\n",
    "    ]\n",
    "\n",
    "    for q in user_queries:\n",
    "        print(\"\\n==============================\")\n",
    "        print(f\"User query: {q}\")\n",
    "\n",
    "        # 1) send to service\n",
    "        service.set_demand(q)\n",
    "\n",
    "        # 2) let the service decide (FILE_QUERY vs QA)\n",
    "        resp = service.get_LLM_reply()\n",
    "\n",
    "        # 3) branch on what the service decided\n",
    "        if resp.get(\"type\") == \"file_query\":\n",
    "            print(\"→ Detected intent: FILE_QUERY\")\n",
    "            print(\"→ Top matched documents:\")\n",
    "            for item in resp.get(\"results\", []):\n",
    "                print(f\"  - {item['title']} (relevance={item['relevance_percent']})\")\n",
    "        else:\n",
    "            print(\"→ Detected intent: QA\")\n",
    "            print(\"→ Answer:\")\n",
    "            print(resp.get(\"reply\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3cdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
