{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeea39ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FILE',\n",
       "  'Designing a RAG-based Question Answering System (relevance=100.00%)\\nEvaluation of RAG vs Vanilla LLM Question Answering (relevance=100.00%)\\nDesign Review Minutes: RAG-based Knowledge Assistant (relevance=100.00%)\\nLLM Application Architecture for Enterprise Search (relevance=91.82%)\\n'),\n",
       " ('QA',\n",
       "  \"RAG-based QA retrieves relevant documents before generating answers, while standard LLM QA relies solely on the model's internal knowledge. RAG reduces hallucinations and improves accuracy by grounding responses in retrieved information [doc_rag_qa_001][doc_eval_004].\"),\n",
       " ('FILE',\n",
       "  'Company Policy and Approval Flow 2025 (relevance=100.00%)\\nLLM Application Architecture for Enterprise Search (relevance=38.57%)\\nDesign Review Minutes: RAG-based Knowledge Assistant (relevance=17.14%)\\nDesigning a RAG-based Question Answering System (relevance=8.57%)\\n'),\n",
       " ('QA', 'No valid reference.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from answer_generator_module.semantic_service import answer_generator\n",
    "answer_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f87d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
